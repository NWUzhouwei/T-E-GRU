{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "identical-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbMask():\n",
    "    def __init__(self, B, H, L, index, scores, device=\"cpu\"):\n",
    "        _mask = torch.ones(L, scores.shape[-1], dytpe=torch.bool).to(device).triu(1)\n",
    "        _mask_ex = _mask[None, None, :].expand(B, H, L, scores.shape[-1])\n",
    "        indicator = _mask_ex[torch.arange(B)[:, None, None],\n",
    "                             torch.arange(H)[None, :, None],\n",
    "                             index, :].to(device)\n",
    "        self._mask = indicator.view(scores.shape).to(device)\n",
    "    \n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask\n",
    "    \n",
    "class TriangularCausalMask():\n",
    "    def __init__(self, B, L, device=\"cpu\"):\n",
    "        mask_shape = [B, 1, L, L]\n",
    "        with torch.no_grad():\n",
    "            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "described-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbAttention(nn.Module):\n",
    "    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1):\n",
    "        super(ProbAttention, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def _prob_QK(self, Q, K, sample_k, n_top):\n",
    "        # Q [B, H, L, D]\n",
    "        B, H, L, E = K.shape\n",
    "        _, _, S, _ = Q.shape\n",
    "\n",
    "        # calculate the sampled Q_K\n",
    "        K_expand = K.unsqueeze(-3).expand(B, H, S, L, E)\n",
    "        indx_sample = torch.randint(L, (S, sample_k))\n",
    "        K_sample = K_expand[:, :, torch.arange(S).unsqueeze(1), indx_sample, :]\n",
    "        Q_K_sample = torch.matmul(Q.unsqueeze(-2), K_sample.transpose(-2, -1)).squeeze()\n",
    "\n",
    "        # find the Top_k query with sparisty measurement\n",
    "        M = Q_K_sample.max(-1)[0] - torch.div(Q_K_sample.sum(-1), L)\n",
    "        M_top = M.topk(n_top, sorted=False)[1]\n",
    "\n",
    "        # use the reduced Q to calculate Q_K\n",
    "        Q_reduce = Q[torch.arange(B)[:, None, None],\n",
    "                     torch.arange(H)[None, :, None],\n",
    "                     M_top, :]\n",
    "        Q_K = torch.matmul(Q_reduce, K.transpose(-2, -1))\n",
    "\n",
    "        return Q_K, M_top\n",
    "\n",
    "    def _get_initial_context(self, V, L_Q):\n",
    "        B, H, L_V, D = V.shape\n",
    "        if not self.mask_flag:\n",
    "            V_sum = V.sum(dim=-2)\n",
    "            contex = V_sum.unsqueeze(-2).expand(B, H, L_Q, V_sum.shape[-1]).clone()\n",
    "        else: # use mask\n",
    "            assert(L_Q == L_V) # requires that L_Q == L_V, i.e. for self-attention only\n",
    "            contex = V.cumsum(dim=-1)\n",
    "        return contex\n",
    "\n",
    "    def _update_context(self, context_in, V, scores, index, L_Q, attn_mask):\n",
    "        B, H, L_V, D = V.shape\n",
    "\n",
    "        if self.mask_flag:\n",
    "            attn_mask = ProbMask(B, H, L_Q, index, scores, device=V.device)\n",
    "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1) # nn.Softmax(dim=-1)(scores)\n",
    "\n",
    "        context_in[torch.arange(B)[:, None, None],\n",
    "                   torch.arange(H)[None, :, None],\n",
    "                   index, :] = torch.matmul(attn, V)\n",
    "        return context_in\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, H, D = queries.shape\n",
    "        _, S, _, _ = keys.shape\n",
    "\n",
    "        queries = queries.view(B, H, L, -1)\n",
    "        keys = keys.view(B, H, S, -1)\n",
    "        values = values.view(B, H, S, -1)\n",
    "\n",
    "        U = self.factor * np.ceil(np.log(S)).astype('int').item()\n",
    "        u = self.factor * np.ceil(np.log(L)).astype('int').item()\n",
    "        \n",
    "        scores_top, index = self._prob_QK(queries, keys, u, U)\n",
    "        # add scale factor\n",
    "        scale = self.scale or 1./sqrt(D)\n",
    "        if scale is not None:\n",
    "            scores_top = scores_top * scale\n",
    "        # get the context\n",
    "        context = self._get_initial_context(values, L)\n",
    "        # update the context with selected top_k queries\n",
    "        context = self._update_context(context, values, scores_top, index, L, attn_mask)\n",
    "        \n",
    "        return context.contiguous()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "strategic-pontiac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads, d_keys=None,\n",
    "                 d_values=None):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model//n_heads)\n",
    "        d_values = d_values or (d_model//n_heads)\n",
    "\n",
    "        self.inner_attention = attention\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "        \n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        out = self.inner_attention(\n",
    "            queries,\n",
    "            keys,\n",
    "            values,\n",
    "            attn_mask\n",
    "        ).view(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "stock-egypt",
   "metadata": {},
   "outputs": [],
   "source": [
    "atten = AttentionLayer(ProbAttention(False, 5, attention_dropout=0.0), 300, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "approved-buffalo",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1,100,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "another-taxation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 100, 300])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atten(x,x,x,None).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-rebound",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abandoned-korea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import jieba\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import random_split,DataLoader\n",
    "import warnings\n",
    "import torch.optim as optim\n",
    "import time\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# 使用gensim加载预训练中文分词embedding, 有可能需要等待1-2分钟\n",
    "cn_model = KeyedVectors.load_word2vec_format('../models/embeddings/sgns.zhihu.bigram', \n",
    "                                             binary=False, unicode_errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "flexible-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdata(filename,num_words = 50000,max_tokens = 90):\n",
    "    data = pd.read_csv(filename).sample(10000)\n",
    "    data = data.to_numpy()\n",
    "    \n",
    "    for item in data:\n",
    "        text = re.sub(\"[\\s+\\/_$%^*(+\\\"\\']+|[+——？、~@#￥%……&*（）]+\", \"\", item[0])\n",
    "        cut = jieba.cut(text)\n",
    "        cut_list = [i for i in cut]\n",
    "        for i, word in enumerate(cut_list):\n",
    "            try:\n",
    "                cut_list[i] = cn_model.vocab[word].index\n",
    "            except:\n",
    "                cut_list[i] = 0\n",
    "        item[0] = np.array(cut_list)\n",
    "        \n",
    "    train_pad = pad_sequences(data[:,0], maxlen=max_tokens,padding='pre', truncating='pre')\n",
    "    train_pad[ train_pad>=num_words] = 0\n",
    "    data_set = [(train_pad[i],data[i][1]) for i in range(len(train_pad))]\n",
    "\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "flush-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_matrix(num_words = 50000,embedding_dim = 300):\n",
    "\n",
    "    # 初始化embedding_matrix\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "    for i in range(num_words):\n",
    "        embedding_matrix[i,:] = cn_model[ cn_model.index2word[i] ]\n",
    "    embedding_matrix = embedding_matrix.astype('float32')\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "frequent-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, batch_size, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.Tensor(embedding_matrix()))\n",
    "        self.embedding.requires_grad = False\n",
    "\n",
    "        self.attention = AttentionLayer(ProbAttention(False, 5, attention_dropout=0.0), hidden_size*num_layers, 8)\n",
    "        self.rnn = nn.GRU(input_size, hidden_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "\n",
    "    def forward(self, x, h):\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x,h = self.rnn(x,h)\n",
    "        \n",
    "        output = self.attention(x,x,x,None)\n",
    "        output = self.linear(output[-1])\n",
    "        output = self.softmax(output)\n",
    "        return output\n",
    "\n",
    "    def initHidden(self):\n",
    "        h_0 = torch.zeros(self.num_layers, self.batch_size, self.hidden_size)\n",
    "        return h_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "greenhouse-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db = getdata(filename = '../data/trainData_60w.csv',num_words = 50000,max_tokens = 100)\n",
    "val_db = getdata(filename = '../data/testData_10w.csv',num_words = 50000,max_tokens = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "accredited-darwin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (embedding): Embedding(50000, 300)\n",
      "  (attention): AttentionLayer(\n",
      "    (inner_attention): ProbAttention(\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (query_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (key_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (value_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (out_projection): Linear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (rnn): GRU(300, 128)\n",
      "  (linear): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (softmax): LogSoftmax(dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "Batch_size = 128\n",
    "N_EPOCHS = 100\n",
    "\n",
    "net = Net(input_size=300, hidden_size=128, output_size=2, batch_size=Batch_size)\n",
    "net = net.to(device)\n",
    "criterion = nn.NLLLoss().to(device)\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "blank-prospect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.005762265324592591, 0.506)\n",
      "(0.0055442614555358885, 0.5221)\n"
     ]
    }
   ],
   "source": [
    "def train(train_db, net, batch_size=20):\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "\n",
    "    data = DataLoader(train_db, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "\n",
    "    for i, (text, label) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        text = text.long().to(device)\n",
    "        label = label.long().to(device)\n",
    "\n",
    "        h = net.initHidden()\n",
    "        h = h.to(device)\n",
    "        output = net(text, h)\n",
    "        loss = criterion(output, label)\n",
    "\n",
    "        train_acc += (label.view(-1, 1) == output.topk(1)[1]).sum().item()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_loss / len(train_db), train_acc / len(train_db)\n",
    "\n",
    "\n",
    "def valid(val_db, net, batch_size=20):\n",
    "    val_loss = 0\n",
    "    val_acc = 0\n",
    "\n",
    "    data = DataLoader(val_db, batch_size=batch_size, shuffle=True,drop_last=True)\n",
    "\n",
    "    for text, label in data:\n",
    "        with torch.no_grad():\n",
    "            text = text.long().to(device)\n",
    "            label = label.long().to(device)\n",
    "\n",
    "            h = net.initHidden()\n",
    "            h = h.to(device)\n",
    "            output = net(text, h)\n",
    "            loss = criterion(output, label)\n",
    "\n",
    "            val_acc += (label.view(-1, 1) == output.topk(1)[1]).sum().item()\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_db), val_acc / len(val_db)\n",
    "print(train(train_db, net, batch_size=Batch_size))\n",
    "print(valid(val_db, net, batch_size=Batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-toolbox",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:sentimentAnalysis]",
   "language": "python",
   "name": "conda-env-sentimentAnalysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
